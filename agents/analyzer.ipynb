{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17253e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# agents/analyzer.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "class Analyzer:\n",
    "    \"\"\"\n",
    "    LLM-based analyzer: generates a short plan/insight text from state,\n",
    "    and produces a fixed-size embedding to pass to the RL recommender.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_name: str = \"distilgpt2\", embed_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\", device: Optional[str] = None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # tokenizer / lm\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "        # ensure pad token exists (some small causal models don't set it)\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            # set pad token to eos token for generation padding safety\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(llm_name).to(self.device)\n",
    "\n",
    "        # embedding model (sentence-transformer style). fallback to None if not available\n",
    "        try:\n",
    "            self.embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "            self.embed_model = AutoModel.from_pretrained(embed_model_name).to(self.device)\n",
    "            self.use_embed_model = True\n",
    "        except Exception:\n",
    "            self.embed_model = None\n",
    "            self.use_embed_model = False\n",
    "\n",
    "    def generate_insight(self, state_repr: str, max_new_tokens: int = 40, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Return a short insight text given a state representation.\"\"\"\n",
    "        prompt = f\"Context: {state_repr}\\nInsight:\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        pad_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
    "        out_ids = self.llm.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=pad_id,\n",
    "        )\n",
    "        # decode and robustly strip prompt prefix\n",
    "        text = self.tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "        if text.startswith(prompt):\n",
    "            insight = text[len(prompt):].strip()\n",
    "        else:\n",
    "            # fallback: try splitting at the last \"Insight:\" marker\n",
    "            insight = text.split(\"Insight:\", 1)[-1].strip()\n",
    "        return insight\n",
    "\n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Return a numpy vector embedding for the given text (float32).\"\"\"\n",
    "        if self.use_embed_model:\n",
    "            toks = self.embed_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.embed_model(**toks, return_dict=True)\n",
    "                # prefer last_hidden_state pooling with attention mask if available\n",
    "                if hasattr(out, \"last_hidden_state\"):\n",
    "                    hidden = out.last_hidden_state  # (1, seq_len, dim)\n",
    "                    mask = toks.get(\"attention_mask\", None)\n",
    "                    if mask is not None:\n",
    "                        mask = mask.unsqueeze(-1).to(hidden.dtype)\n",
    "                        summed = (hidden * mask).sum(dim=1)\n",
    "                        denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "                        emb = (summed / denom).squeeze().cpu().numpy()\n",
    "                    else:\n",
    "                        emb = hidden.mean(dim=1).squeeze().cpu().numpy()\n",
    "                else:\n",
    "                    emb = getattr(out, \"pooler_output\", None)\n",
    "                    if emb is None:\n",
    "                        raise RuntimeError(\"Embedding model returned no usable output\")\n",
    "                    emb = emb.squeeze().cpu().numpy()\n",
    "            return emb.astype(np.float32)\n",
    "        else:\n",
    "            # fallback cheap embedding if no embed model available\n",
    "            toks = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "            ids = toks[\"input_ids\"].squeeze().cpu().numpy()\n",
    "            vec = np.zeros(128, dtype=np.float32)\n",
    "            vec[:min(len(ids), 128)] = ids[:128] / 10000.0\n",
    "            return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "441fe8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Dans __init__\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mself\u001b[39m.llm = AutoModelForCausalLM.from_pretrained(\u001b[43mllm_name\u001b[49m).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.llm = PeftModel.from_pretrained(\u001b[38;5;28mself\u001b[39m.llm, \u001b[33m\"\u001b[39m\u001b[33mlora_analyzer\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# In generate_insight method\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'llm_name' is not defined"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Dans __init__\n",
    "self.llm = AutoModelForCausalLM.from_pretrained(llm_name).to(self.device)\n",
    "self.llm = PeftModel.from_pretrained(self.llm, \"lora_analyzer\").to(self.device)\n",
    "\n",
    "# In generate_insight method\n",
    "self.llm.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "ref = \"Réduire les coûts logistiques et négocier avec les fournisseurs.\"\n",
    "gen = analyzer.generate_insight(\"Les dépenses logistiques augmentent de 20% ce trimestre.\")\n",
    "cosine = util.cos_sim(embedder.encode(ref), embedder.encode(gen))\n",
    "print(\"Similarity:\", cosine.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
