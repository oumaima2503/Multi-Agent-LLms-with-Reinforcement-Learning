{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/magrpo_train.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MAGRPOTrainer:\n",
    "    def __init__(self, analyzer, recommender, validator, env, buffer, gamma=0.99, lam=0.95, clip_eps=0.2, epochs=4, batch_size=32, device=None):\n",
    "        self.analyzer = analyzer\n",
    "        self.recommender = recommender\n",
    "        self.validator = validator\n",
    "        self.env = env\n",
    "        self.buffer = buffer\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_eps = clip_eps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def collect_episode(self, max_steps=50):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done and steps < max_steps:\n",
    "            state_repr = state.tolist() if hasattr(state, 'tolist') else str(state)\n",
    "            insight = self.analyzer.generate_insight(state_repr)\n",
    "            plan_emb = self.analyzer.embed_text(insight)\n",
    "            # convert to tensors\n",
    "            obs_t = torch.tensor(state, dtype=torch.float32)\n",
    "            plan_emb_t = torch.tensor(plan_emb, dtype=torch.float32)\n",
    "            action, logprob, value = self.recommender.act(obs_t, plan_emb_t)\n",
    "            next_state, env_reward, done, _ = self.env.step(action)\n",
    "            shaped = self.validator.shaped_reward(env_reward, insight, action, tokens_used=len(insight.split()))\n",
    "            transition = {\n",
    "                'state': state,\n",
    "                'plan_text': insight,\n",
    "                'plan_emb': plan_emb,\n",
    "                'action': action,\n",
    "                'reward': shaped,\n",
    "                'value': value,\n",
    "                'logprob': logprob,\n",
    "                'next_state': next_state,\n",
    "                'done': done\n",
    "            }\n",
    "            self.buffer.store(transition)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        rewards = np.array(rewards)\n",
    "        values = np.array(values + [0.0])\n",
    "        dones = np.array(dones)\n",
    "        gae = 0.0\n",
    "        returns = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            mask = 1.0 - float(dones[step])\n",
    "            delta = rewards[step] + self.gamma * values[step+1] * mask - values[step]\n",
    "            gae = delta + self.gamma * self.lam * mask * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        advs = np.array(returns) - values[:-1]\n",
    "        return np.array(returns), (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "\n",
    "    def update(self):\n",
    "        transitions = self.buffer.get_all()\n",
    "        if len(transitions) == 0:\n",
    "            return\n",
    "        states = np.stack([t['state'] for t in transitions])\n",
    "        plan_embs = np.stack([t['plan_emb'] for t in transitions])\n",
    "        actions = np.array([t['action'] for t in transitions])\n",
    "        rewards = [t['reward'] for t in transitions]\n",
    "        values = [t['value'] for t in transitions]\n",
    "        logprobs = np.array([t['logprob'] for t in transitions])\n",
    "        dones = [t['done'] for t in transitions]\n",
    "\n",
    "        returns, advs = self.compute_gae(rewards, values, dones)\n",
    "        # convert to torch\n",
    "        obs = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        plan_emb = torch.tensor(plan_embs, dtype=torch.float32).to(self.device)\n",
    "        actions_t = torch.tensor(actions).to(self.device)\n",
    "        old_logprobs = torch.tensor(logprobs, dtype=torch.float32).to(self.device)\n",
    "        returns_t = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "        advs_t = torch.tensor(advs, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        n = len(actions)\n",
    "        inds = np.arange(n)\n",
    "        for _ in range(self.epochs):\n",
    "            np.random.shuffle(inds)\n",
    "            for start in range(0, n, self.batch_size):\n",
    "                mb_idx = inds[start:start+self.batch_size]\n",
    "                mb_obs = obs[mb_idx]\n",
    "                mb_plan = plan_emb[mb_idx]\n",
    "                mb_actions = actions_t[mb_idx]\n",
    "                mb_old_logp = old_logprobs[mb_idx]\n",
    "                mb_returns = returns_t[mb_idx]\n",
    "                mb_advs = advs_t[mb_idx]\n",
    "\n",
    "                logits, vals = self.recommender.net(mb_obs, mb_plan)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_logp = dist.log_prob(mb_actions)\n",
    "                ratio = torch.exp(new_logp - mb_old_logp)\n",
    "                surr1 = ratio * mb_advs\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * mb_advs\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.mse_loss(vals, mb_returns)\n",
    "                entropy = dist.entropy().mean()\n",
    "                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "                self.recommender.update(loss)\n",
    "\n",
    "        self.buffer.clear()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
